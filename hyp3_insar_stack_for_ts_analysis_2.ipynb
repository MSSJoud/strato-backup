{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d20494ef",
   "metadata": {},
   "source": [
    "# Time series analysis with HyP3 and MintPy\n",
    "\n",
    "This notebook walks through performing a time-series analysis of the 2019 Ridgecrest, CA earthquake with On Demand InSAR products from the Alaska Satellite facility and MintPy. We'll:\n",
    "\n",
    "1. Use the [ASF Search Python package](https://docs.asf.alaska.edu/asf_search/basics/) to:\n",
    "   - Search ASF's catalog for Sentinel-1 SAR products covering the [Ridgecrest earthquake](https://earthquake.usgs.gov/storymap/index-ridgecrest.html)\n",
    "   - Select a reference scene to generate a baseline stack\n",
    "   - Select a [short baseline subset (SBAS)](https://docs.asf.alaska.edu/vertex/sbas/) of scene pairs for InSAR processing\n",
    "\n",
    "\n",
    "2. Use the [HyP3 Python SDK](https://hyp3-docs.asf.alaska.edu/using/sdk/) to:\n",
    "   - Request On Demand InSAR products from ASF HyP3\n",
    "   - Download the InSAR products when they are done processing\n",
    "\n",
    "\n",
    "3. Use [GDAL](https://gdal.org/api/index.html#python-api) and [MintPy](https://mintpy.readthedocs.io/en/latest/) to:\n",
    "   - Prepare the InSAR products for MintPy\n",
    "   - perform a time-series analysis with MintPy\n",
    "   \n",
    "---\n",
    "\n",
    "**Note:** This notebook does assume you have some familiarity with InSAR processing with MintPy already, and is a minimal example without much context or explanations. If you're new to InSAR and MintPy, I suggest checking out:\n",
    "* our [InSAR on Demand Story Map](https://storymaps.arcgis.com/stories/68a8a3253900411185ae9eb6bb5283d3)\n",
    "\n",
    "\n",
    "* [OpenSARlab's](https://opensarlab-docs.asf.alaska.edu/) highly detailed walkthrough of using HyP3 + MintPy via these notebooks:\n",
    "  * [Prepare a HyP3 InSAR Stack for MintPy](https://nbviewer.org/github/ASFOpenSARlab/opensarlab-notebooks/blob/master/SAR_Training/English/Master/Prepare_HyP3_InSAR_Stack_for_MintPy.ipynb)\n",
    "  * [MintPy Time-series Analysis](https://nbviewer.org/github/ASFOpenSARlab/opensarlab-notebooks/blob/master/SAR_Training/English/Master/MintPy_Time_Series_From_Prepared_Data_Stack.ipynb)\n",
    "  \n",
    "    Note: While these notebooks make some assumptions you're working in OpenSARlab, you can run these \n",
    "    notebooks outside OpenSARlab by creating [this conda environment](https://github.com/ASFOpenSARlab/opensarlab-envs/blob/main/Environment_Configs/insar_analysis_env.yml)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a6e353",
   "metadata": {},
   "source": [
    "## 0. Initial Setup\n",
    "\n",
    "To run this notebook, you'll need a conda environment with the required dependencies. You can set up a new environment (recommended) and run the jupyter server like:\n",
    "```shell\n",
    "conda create -n hyp3-mintpy python=3.10 \"asf_search>=7.0.0\" hyp3_sdk \"mintpy>=1.5.2\" pandas jupyter ipympl\n",
    "\n",
    "conda activate hyp3-mintpy\n",
    "jupyter notebook hyp3_insar_stack_for_ts_analysis.ipynb\n",
    "```\n",
    "Or, install these dependencies into your own environment:\n",
    "```shell\n",
    "conda install python=3.10 \"asf_search>=7.0.0\" hyp3_sdk \"mintpy>=1.5.2\" pandas jupyter ipympl\n",
    "\n",
    "jupyter notebook hyp3_insar_stack_for_ts_analysis.ipynb\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85677220",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'flightdirection'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/insar/lib/python3.13/site-packages/pandas/core/indexes/base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3811\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7096\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'flightdirection'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     12\u001b[39m df.columns = df.columns.str.strip().str.lower()\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Split by flightDirection\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m df_asc = df[\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mflightdirection\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m.str.lower() == \u001b[33m'\u001b[39m\u001b[33mascending\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     16\u001b[39m df_desc = df[df[\u001b[33m'\u001b[39m\u001b[33mflightdirection\u001b[39m\u001b[33m'\u001b[39m].str.lower() == \u001b[33m'\u001b[39m\u001b[33mdescending\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Save separate files\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/insar/lib/python3.13/site-packages/pandas/core/frame.py:4107\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4105\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4106\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4107\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4108\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4109\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/insar/lib/python3.13/site-packages/pandas/core/indexes/base.py:3819\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3814\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3815\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3816\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3817\u001b[39m     ):\n\u001b[32m   3818\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3819\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3820\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3821\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3822\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3823\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3824\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'flightdirection'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Define paths\n",
    "csv_path = Path(\"/home/ubuntu/work/aoi_bologna/aoi_bologna_slcs_2018_2024.csv\")\n",
    "output_dir = csv_path.parent\n",
    "\n",
    "# Read the full SLC CSV\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Normalize column names (in case there's a case mismatch)\n",
    "df.columns = df.columns.str.strip().str.lower()\n",
    "\n",
    "# Split by flightDirection\n",
    "df_asc = df[df['flightdirection'].str.lower() == 'ascending']\n",
    "df_desc = df[df['flightdirection'].str.lower() == 'descending']\n",
    "\n",
    "# Save separate files\n",
    "asc_path = output_dir / \"slcs_asc.csv\"\n",
    "desc_path = output_dir / \"slcs_desc.csv\"\n",
    "\n",
    "df_asc.to_csv(asc_path, index=False)\n",
    "df_desc.to_csv(desc_path, index=False)\n",
    "\n",
    "print(f\"Saved:\\n - Ascending → {asc_path}\\n - Descending → {desc_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e566f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from dateutil.parser import parse as parse_date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33543149",
   "metadata": {},
   "source": [
    "### Set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa17bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_name = '2019_ridgecrest'\n",
    "work_dir = Path.cwd() / project_name\n",
    "data_dir = work_dir / 'data'\n",
    "\n",
    "stack_start = parse_date('2019-06-09 00:00:00Z')\n",
    "stack_end = parse_date('2019-08-19 00:00:00Z')\n",
    "max_temporal_baseline = 24 #days\n",
    "\n",
    "data_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e62c544",
   "metadata": {},
   "source": [
    "## 1. Select InSAR pairs with ASF Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4498c66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asf_search as asf\n",
    "import pandas as pd\n",
    "\n",
    "search_results = asf.geo_search(\n",
    "        platform=asf.PLATFORM.SENTINEL1,\n",
    "        intersectsWith='POINT(-117.55 35.77)',\n",
    "        start='2019-06-09',\n",
    "        end='2019-08-19',\n",
    "        processingLevel=asf.PRODUCT_TYPE.SLC,\n",
    "        beamMode=asf.BEAMMODE.IW,\n",
    "        flightDirection=asf.FLIGHT_DIRECTION.DESCENDING,\n",
    "        relativeOrbit=71\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7ccdcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_results = asf.baseline_search.stack_from_product(search_results[-1])\n",
    "\n",
    "columns = list(baseline_results[0].properties.keys()) + ['geometry', ]\n",
    "data = [list(scene.properties.values()) + [scene.geometry, ] for scene in baseline_results]\n",
    "\n",
    "stack = pd.DataFrame(data, columns=columns)\n",
    "stack['startTime'] = stack.startTime.apply(parse_date)\n",
    "\n",
    "stack = stack.loc[(stack_start <= stack.startTime) & (stack.startTime <= stack_end)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d9f38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sbas_pairs = set()\n",
    "\n",
    "for reference, rt in stack.loc[::-1, ['sceneName', 'temporalBaseline']].itertuples(index=False):\n",
    "    secondaries = stack.loc[\n",
    "        (stack.sceneName != reference)\n",
    "        & (stack.temporalBaseline - rt <= max_temporal_baseline)\n",
    "        & (stack.temporalBaseline - rt > 0)\n",
    "    ]\n",
    "    for secondary in secondaries.sceneName:\n",
    "        sbas_pairs.add((reference, secondary))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5c5b0b",
   "metadata": {},
   "source": [
    "## 2. Request On Demand InSAR products from ASF HyP3\n",
    "\n",
    "Use your [NASA Earthdata login](https://urs.earthdata.nasa.gov/) to connect to [ASF HyP3](https://hyp3-docs.asf.alaska.edu/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be78f415",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hyp3_sdk as sdk\n",
    "\n",
    "hyp3 = sdk.HyP3(prompt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1dec3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs = sdk.Batch()\n",
    "for reference, secondary in sbas_pairs:\n",
    "    jobs += hyp3.submit_insar_job(reference, secondary, name=project_name,\n",
    "                                  include_dem=True, include_look_vectors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b82d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs = hyp3.watch(jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b3d2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs = hyp3.find_jobs(name=project_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b461cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "insar_products = jobs.download_files(data_dir)\n",
    "insar_products = [sdk.util.extract_zipped_product(ii) for ii in insar_products]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3181031f",
   "metadata": {},
   "source": [
    "## 3. Time-series Analysis with MintPy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd1b850",
   "metadata": {},
   "source": [
    "### 3.1 Subset all GeoTIFFs to their common overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f75d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import List, Union\n",
    "from osgeo import gdal\n",
    "\n",
    "\n",
    "def get_common_overlap(file_list: List[Union[str, Path]]) -> List[float]:\n",
    "    \"\"\"Get the common overlap of  a list of GeoTIFF files\n",
    "    \n",
    "    Arg:\n",
    "        file_list: a list of GeoTIFF files\n",
    "    \n",
    "    Returns:\n",
    "         [ulx, uly, lrx, lry], the upper-left x, upper-left y, lower-right x, and lower-right y\n",
    "         corner coordinates of the common overlap\n",
    "    \"\"\"\n",
    "    \n",
    "    corners = [gdal.Info(str(dem), format='json')['cornerCoordinates'] for dem in file_list]\n",
    "\n",
    "    ulx = max(corner['upperLeft'][0] for corner in corners)\n",
    "    uly = min(corner['upperLeft'][1] for corner in corners)\n",
    "    lrx = min(corner['lowerRight'][0] for corner in corners)\n",
    "    lry = max(corner['lowerRight'][1] for corner in corners)\n",
    "    return [ulx, uly, lrx, lry]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c55f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = data_dir.glob('*/*_dem.tif')\n",
    "\n",
    "overlap = get_common_overlap(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d94460",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import List, Union\n",
    "\n",
    "def clip_hyp3_products_to_common_overlap(data_dir: Union[str, Path], overlap: List[float]) -> None:\n",
    "    \"\"\"Clip all GeoTIFF files to their common overlap\n",
    "    \n",
    "    Args:\n",
    "        data_dir:\n",
    "            directory containing the GeoTIFF files to clip\n",
    "        overlap:\n",
    "            a list of the upper-left x, upper-left y, lower-right-x, and lower-tight y\n",
    "            corner coordinates of the common overlap\n",
    "    Returns: None\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    files_for_mintpy = ['_water_mask.tif', '_corr.tif', '_unw_phase.tif', '_dem.tif', '_lv_theta.tif', '_lv_phi.tif']\n",
    "\n",
    "    for extension in files_for_mintpy:\n",
    "\n",
    "        for file in data_dir.rglob(f'*{extension}'):\n",
    "\n",
    "            dst_file = file.parent / f'{file.stem}_clipped{file.suffix}'\n",
    "\n",
    "            gdal.Translate(destName=str(dst_file), srcDS=str(file), projWin=overlap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023ca045",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_hyp3_products_to_common_overlap(data_dir, overlap)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92be356f",
   "metadata": {},
   "source": [
    "### 3.2 Create the MintPy config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3cf17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mintpy_config = work_dir / 'mintpy_config.txt'\n",
    "mintpy_config.write_text(\n",
    "f\"\"\"\n",
    "mintpy.load.processor        = hyp3\n",
    "##---------interferogram datasets\n",
    "mintpy.load.unwFile          = {data_dir}/*/*_unw_phase_clipped.tif\n",
    "mintpy.load.corFile          = {data_dir}/*/*_corr_clipped.tif\n",
    "##---------geometry datasets:\n",
    "mintpy.load.demFile          = {data_dir}/*/*_dem_clipped.tif\n",
    "mintpy.load.incAngleFile     = {data_dir}/*/*_lv_theta_clipped.tif\n",
    "mintpy.load.azAngleFile      = {data_dir}/*/*_lv_phi_clipped.tif\n",
    "mintpy.load.waterMaskFile    = {data_dir}/*/*_water_mask_clipped.tif\n",
    "# mintpy.troposphericDelay.method = no\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87385631",
   "metadata": {},
   "source": [
    "### 3.3 run MintPy to do the time series analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a012c642",
   "metadata": {},
   "outputs": [],
   "source": [
    "!smallbaselineApp.py --dir {work_dir} {mintpy_config}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e866ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "from mintpy.cli import view, tsview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf5cfdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "view.main([f'{work_dir}/velocity.h5'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6172aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsview.main([f'{work_dir}/timeseries.h5'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a858c62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "insar",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
