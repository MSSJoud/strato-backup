{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46e3232",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from dateutil.parser import parse as parse_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab159bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Define project paths ===\n",
    "project_name = \"aoi_3_bologna\"\n",
    "\n",
    "# Directory where HyP3 products (SLCs/interferograms) are stored\n",
    "data_dir = Path(\"/mnt/data\") / project_name\n",
    "hyp_int_dir = data_dir / \"hyp_int\"  # For interferograms only\n",
    "\n",
    "# Working directory for MintPy, config files, etc.\n",
    "work_dir = Path.home() / \"work\" / \"aoi_bologna\" / \"aoi_3_1\"\n",
    "\n",
    "# Create necessary folders if they don't exist\n",
    "for d in [data_dir, hyp_int_dir, work_dir]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"‚úîÔ∏è Project setup complete:\")\n",
    "print(f\"  - Data directory:      {data_dir}\")\n",
    "print(f\"  - HyP3 interferograms: {hyp_int_dir}\")\n",
    "print(f\"  - Work directory:      {work_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b0b48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define search configuration ---\n",
    "stack_start = parse_date(\"2016-12-31T23:00:00Z\")\n",
    "stack_end = parse_date(\"2025-06-30T21:59:59Z\")\n",
    "max_temporal_baseline = 24  # days, used later for SBAS pair filtering (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978af5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asf_search as asf\n",
    "from pprint import pprint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f04615",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define ASF search parameters\n",
    "opts = asf.ASFSearchOptions(**{\n",
    "    \"maxResults\": 5000,\n",
    "    \"bbox\": [11.2, 44.4, 11.5, 44.6],  # Bologna AOI (EPSG:32633)\n",
    "    \"beamSwath\": [\"IW\"],\n",
    "    \"flightDirection\": \"DESCENDING\",\n",
    "    \"polarization\": [\"VV+VH\", \"VV\"],\n",
    "    \"processingLevel\": [\"SLC\"],\n",
    "    \"start\": stack_start.isoformat(),\n",
    "    \"end\": stack_end.isoformat(),\n",
    "    \"dataset\": [\"SENTINEL-1\"]\n",
    "})\n",
    "\n",
    "# Search SLC scenes from ASF\n",
    "search_results = asf.search(opts=opts)\n",
    "print(f\"üîç Found {len(search_results)} scenes.\")\n",
    "\n",
    "# Create baseline stack (SBAS-compatible) from the most recent scene\n",
    "baseline_results = asf.baseline_search.stack_from_product(search_results[-1])\n",
    "print(f\"Generated {len(baseline_results)} SBAS pairs.\")\n",
    "\n",
    "# Preview first few SBAS pairs\n",
    "pprint(baseline_results[:3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402026f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Dynamically get all properties\n",
    "columns = list(baseline_results[0].properties.keys()) + ['geometry']\n",
    "data = [list(scene.properties.values()) + [scene.geometry] for scene in baseline_results]\n",
    "\n",
    "# Create DataFrame\n",
    "stack = pd.DataFrame(data, columns=columns)\n",
    "\n",
    "# Show available columns to inspect\n",
    "print(\"Available DataFrame columns:\")\n",
    "print(stack.columns.tolist())\n",
    "\n",
    "# Convert to datetime\n",
    "stack['startTime'] = stack['startTime'].apply(parse_date)\n",
    "\n",
    "# Filter by date range\n",
    "stack = stack.loc[(stack_start <= stack.startTime) & (stack.startTime <= stack_end)]\n",
    "\n",
    "# Show preview using safe subset\n",
    "preview_cols = [col for col in ['sceneName', 'startTime', 'stopTime'] if col in stack.columns]\n",
    "print(f\"Filtered SBAS stack: {len(stack)} pairs\")\n",
    "stack[preview_cols].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd70cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sbas_pairs = set()\n",
    "\n",
    "for reference, rt in stack.loc[::-1, ['sceneName', 'temporalBaseline']].itertuples(index=False):\n",
    "    secondaries = stack.loc[\n",
    "        (stack.sceneName != reference)\n",
    "        & (stack.temporalBaseline - rt <= max_temporal_baseline)\n",
    "        & (stack.temporalBaseline - rt > 0)\n",
    "    ]\n",
    "    for secondary in secondaries.sceneName:\n",
    "        sbas_pairs.add((reference, secondary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe279bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Save stack metadata (SLCs with all properties)\n",
    "stack_csv_path = work_dir / \"stack_scenes.csv\"\n",
    "stack_json_path = work_dir / \"stack_scenes.json\"\n",
    "\n",
    "stack.to_csv(stack_csv_path, index=False)\n",
    "stack.to_json(stack_json_path, orient=\"records\", indent=2)\n",
    "\n",
    "# Save SBAS pairs\n",
    "sbas_pairs_list = [{\"reference\": ref, \"secondary\": sec} for ref, sec in sbas_pairs]\n",
    "sbas_csv_path = work_dir / \"sbas_pairs.csv\"\n",
    "sbas_json_path = work_dir / \"sbas_pairs.json\"\n",
    "\n",
    "pd.DataFrame(sbas_pairs_list).to_csv(sbas_csv_path, index=False)\n",
    "with open(sbas_json_path, \"w\") as f:\n",
    "    json.dump(sbas_pairs_list, f, indent=2)\n",
    "\n",
    "print(f\"Saved stack and SBAS pairs to: {work_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57225b6",
   "metadata": {},
   "source": [
    "## 2. Request On Demand InSAR products from ASF HyP3\n",
    "\n",
    "# Use your [NASA Earthdata login](https://urs.earthdata.nasa.gov/) to connect to [ASF HyP3](https://hyp3-docs.asf.alaska.edu/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213b0a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hyp3_sdk as sdk\n",
    "hyp3 = sdk.HyP3()  # Automatically uses ~/.netrc\n",
    "\n",
    "# Initialize HyP3 with interactive login if needed\n",
    "# hyp3 = sdk.HyP3(prompt=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca33003",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "import hyp3_sdk as sdk\n",
    "\n",
    "# Initialize\n",
    "hyp3 = sdk.HyP3()\n",
    "jobs = sdk.Batch()\n",
    "\n",
    "# Submit with progress bar\n",
    "for reference, secondary in tqdm(sbas_pairs, desc=\"Submitting InSAR jobs\"):\n",
    "    jobs += hyp3.submit_insar_job(\n",
    "        reference, secondary,\n",
    "        name=project_name,\n",
    "        include_dem=True,\n",
    "        include_look_vectors=True,\n",
    "        include_wrapped_phase=True,\n",
    "        include_los_displacement=True,\n",
    "        include_displacement_maps=True\n",
    "    )\n",
    "    time.sleep(0.2)  # Avoid rate limiting\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2c3374",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "jobs = hyp3.find_jobs()  # This lists all your jobs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f4f6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the jobs object to a file for later use\n",
    "import pickle\n",
    "\n",
    "# Set the path to your target file inside work_dir\n",
    "job_pickle_path = work_dir / 'hyp3_jobs.pkl'\n",
    "\n",
    "# jobs is your HyP3JobList object from hyp3.find_jobs()\n",
    "with open(job_pickle_path, 'wb') as f:\n",
    "    pickle.dump(jobs, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce245f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch all your jobs\n",
    "\n",
    "# Show keys of the first job's `job_parameters` safely\n",
    "first_job = jobs[0]\n",
    "\n",
    "first_job_dict = first_job.to_dict()\n",
    "\n",
    "print(\"Top-level job keys:\")\n",
    "print(first_job_dict.keys())\n",
    "\n",
    "print(\"\\nJob parameter keys:\")\n",
    "print(first_job_dict['job_parameters'].keys())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2891e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_jobs = hyp3.find_jobs(name=\"aoi_3_bologna\")\n",
    "print(f\"Found {len(all_jobs)} jobs\")\n",
    "statuses = [job.status_code for job in all_jobs]\n",
    "from collections import Counter\n",
    "print(Counter(statuses))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104fa308",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_project_jobs = [job for job in all_jobs \n",
    "                   if job.job_type == \"INSAR_GAMMA\" and job.name == project_name]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87cf15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(job.status_code for job in my_project_jobs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5201b06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{first_job.credit_cost}\")\n",
    "\n",
    "total_credits = sum(j.credit_cost for j in my_project_jobs)\n",
    "print(f\"Total credit cost for 'aoi_3_bologna': {total_credits}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ddb36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for job in my_project_jobs:\n",
    "    print(f\"{job.job_id} ‚Äî {job.job_type} ‚Äî {job.status_code} ‚Äî {job.name} ‚Äî {job.request_time}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2ea9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for job in my_project_jobs:\n",
    "    granules = job.job_parameters['granules']\n",
    "    print(f\"{granules[0]} <---> {granules[1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad5df6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "pprint.pprint(my_project_jobs[0].job_parameters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3e88b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Only keep succeeded jobs\n",
    "succeeded_jobs = [job for job in my_project_jobs if job.status_code == 'SUCCEEDED']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5d9c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "downloaded_files = []\n",
    "\n",
    "for job in tqdm(succeeded_jobs, desc=\"Downloading SUCCEEDED HyP3 jobs\"):\n",
    "    downloaded = job.download_files(data_dir)\n",
    "    downloaded_files.extend(downloaded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f7ce4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "records = []\n",
    "\n",
    "for job in succeeded_jobs:\n",
    "    # Get ZIP file name from job\n",
    "    zip_path = os.path.join(data_dir, job.files[0]['filename'])\n",
    "    \n",
    "    if not os.path.exists(zip_path):\n",
    "        continue  # skip if not downloaded\n",
    "    \n",
    "    zip_size = os.path.getsize(zip_path) / (1024**2)  # size in MB\n",
    "    \n",
    "    # Extract dates from granules\n",
    "    granules = job.job_parameters['granules']\n",
    "    reference = granules[0]\n",
    "    secondary = granules[1]\n",
    "    ref_date = reference.split('_')[5][:8]\n",
    "    sec_date = secondary.split('_')[5][:8]\n",
    "\n",
    "    # Compose interferogram basename\n",
    "    intf_name = f\"{ref_date}_{sec_date}\"\n",
    "    \n",
    "    records.append({\n",
    "        \"job_id\": job.job_id,\n",
    "        \"name\": job.name,\n",
    "        \"status\": job.status_code,\n",
    "        \"request_time\": job.request_time,\n",
    "        \"credit_cost\": job.credit_cost,\n",
    "        \"granule_1\": reference,\n",
    "        \"granule_2\": secondary,\n",
    "        \"ref_date\": ref_date,\n",
    "        \"sec_date\": sec_date,\n",
    "        \"job_type\": job.job_type,\n",
    "        \"looks\": job.job_parameters.get(\"looks\"),\n",
    "        \"include_dem\": job.job_parameters.get(\"include_dem\"),\n",
    "        \"include_look_vectors\": job.job_parameters.get(\"include_look_vectors\"),\n",
    "        \"include_displacement_maps\": job.job_parameters.get(\"include_displacement_maps\"),\n",
    "        \"include_los_displacement\": job.job_parameters.get(\"include_los_displacement\"),\n",
    "        \"include_wrapped_phase\": job.job_parameters.get(\"include_wrapped_phase\"),\n",
    "        \"apply_water_mask\": job.job_parameters.get(\"apply_water_mask\"),\n",
    "        \"phase_filter_parameter\": job.job_parameters.get(\"phase_filter_parameter\"),\n",
    "        \"zip_filename\": os.path.basename(zip_path),\n",
    "        \"zip_file_size_MB\": round(zip_size, 2),\n",
    "        \"interferogram_name\": intf_name\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame and save\n",
    "df = pd.DataFrame.from_records(records)\n",
    "df.to_csv(work_dir / \"hyp3_interferogram_jobs.csv\", index=False)\n",
    "\n",
    "print(f\"Saved metadata for {len(df)} jobs to hyp3_interferogram_jobs.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733d595c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "unzipped_dirs = []\n",
    "\n",
    "for job in tqdm(succeeded_jobs, desc=\"Unzipping InSAR products\"):\n",
    "    zip_path = os.path.join(data_dir, job.files[0]['filename'])\n",
    "\n",
    "    if not os.path.exists(zip_path):\n",
    "        continue  # Skip if ZIP not downloaded\n",
    "\n",
    "    unzip_folder = os.path.splitext(zip_path)[0]\n",
    "    if os.path.exists(unzip_folder):\n",
    "        print(f\"Already unzipped: {unzip_folder}\")\n",
    "        unzipped_dirs.append(unzip_folder)\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        extracted_path = sdk.util.extract_zipped_product(zip_path)\n",
    "        unzipped_dirs.append(extracted_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to unzip {zip_path}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046fb31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove zipped files - after checking the zipping pocess was complete \n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from zipfile import ZipFile\n",
    "\n",
    "from tqdm import tqdm\n",
    "from hyp3_sdk.util import extract_zipped_product\n",
    "\n",
    "# Assuming `data_dir` is already defined\n",
    "zipped_files = sorted(Path(data_dir).glob(\"*.zip\"))\n",
    "\n",
    "print(f\"Found {len(zipped_files)} .zip files in {data_dir}\")\n",
    "\n",
    "for zip_path in tqdm(zipped_files, desc=\"Processing ZIPs\"):\n",
    "    zip_path = Path(zip_path)\n",
    "    # Expected unzipped folder name (same as zip without .zip)\n",
    "    unzipped_folder = zip_path.with_suffix('')\n",
    "\n",
    "    if unzipped_folder.exists():\n",
    "        # Already unzipped, safe to delete zip\n",
    "        zip_path.unlink()\n",
    "    else:\n",
    "        # Not yet unzipped ‚Üí unzip then delete zip\n",
    "        try:\n",
    "            extract_zipped_product(zip_path)\n",
    "            zip_path.unlink()\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to extract {zip_path.name}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ec5186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate ZIP file sizes and add to CSV\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# === Paths ===\n",
    "csv_path = work_dir / \"hyp3_interferogram_jobs.csv\"  # already defined\n",
    "data_dir = Path(\"/mnt/data/aoi_3_bologna\")       # already defined\n",
    "\n",
    "# === Load CSV ===\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# === Initialize size column ===\n",
    "zip_sizes_mb = []\n",
    "missing_files = []\n",
    "\n",
    "for fname in df['zip_filename']:  # adjust column name if needed\n",
    "    zip_path = data_dir / fname\n",
    "    if zip_path.exists():\n",
    "        size_mb = zip_path.stat().st_size / (1024 ** 2)\n",
    "        zip_sizes_mb.append(size_mb)\n",
    "    else:\n",
    "        zip_sizes_mb.append(None)\n",
    "        missing_files.append(fname)\n",
    "\n",
    "# === Add column and save ===\n",
    "df['zip_size_MB'] = zip_sizes_mb\n",
    "df.to_csv(csv_path, index=False)\n",
    "\n",
    "# === Summary ===\n",
    "total_size_mb = sum(size for size in zip_sizes_mb if size is not None)\n",
    "print(f\"‚úÖ Total ZIP size: {total_size_mb:.2f} MB\")\n",
    "if missing_files:\n",
    "    print(f\"‚ö†Ô∏è Missing ZIP files: {len(missing_files)}\")\n",
    "    for f in missing_files:\n",
    "        print(\" -\", f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee61db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspecting a specific job object\n",
    "job = succeeded_jobs[0]  # or any index\n",
    "\n",
    "# Check object type\n",
    "print(\"Type of job:\", type(job))\n",
    "\n",
    "# See all top-level attributes\n",
    "print(\"\\nTop-level attributes:\")\n",
    "print(job.__dict__.keys())\n",
    "\n",
    "# See the full object using pprint\n",
    "from pprint import pprint\n",
    "pprint(job.__dict__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aaadc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download GPS time series for a specific station\n",
    "import requests\n",
    "from pathlib import Path\n",
    "from io import StringIO\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# === Setup ===\n",
    "station = \"MEDI\"\n",
    "gps_dir = data_dir / \"gps\"\n",
    "gps_dir.mkdir(exist_ok=True)\n",
    "gps_path = gps_dir / f\"{station}.EU.tenv3\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32833025",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === Download ===\n",
    "url = f\"https://geodesy.unr.edu/gps_timeseries/tenv3/plates/EU/{station}.EU.tenv3\"\n",
    "\n",
    "response = requests.get(url)\n",
    "response.raise_for_status()\n",
    "with open(gps_path, 'w') as f:\n",
    "    f.write(response.text)\n",
    "\n",
    "print(f\"‚úÖ Downloaded {station} time series to {gps_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22e432c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Read and process GPS data ===\n",
    "import pandas as pd\n",
    "\n",
    "# Define correct column names based on your structure\n",
    "col_names = [\n",
    "    \"site\", \"YYMMMDD\", \"decimal_year\", \"MJD\", \"week\", \"doy\",\n",
    "    \"reflon\", \"e0\", \"east\", \"n0\", \"north\", \"u0\", \"up\", \"ant\",\n",
    "    \"sig_e\", \"sig_n\", \"sig_u\", \"corr_en\", \"corr_eu\", \"corr_nu\",\n",
    "    \"lat\", \"lon\", \"height\"\n",
    "]\n",
    "\n",
    "# Read fixed-width file with manual column names\n",
    "df_gps = pd.read_csv(gps_path, delim_whitespace=True, comment=\"#\", names=col_names)\n",
    "\n",
    "# Convert to numeric\n",
    "df_gps['decimal_year'] = pd.to_numeric(df_gps['decimal_year'], errors='coerce')\n",
    "df_gps['up'] = pd.to_numeric(df_gps['up'], errors='coerce')\n",
    "df_gps['sig_u'] = pd.to_numeric(df_gps['sig_u'], errors='coerce')\n",
    "\n",
    "# Drop rows with missing time or up values\n",
    "df_gps = df_gps.dropna(subset=['decimal_year', 'up', 'sig_u'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab74099",
   "metadata": {},
   "outputs": [],
   "source": [
    "lat = df_gps['lat'].iloc[0]\n",
    "lon = df_gps['lon'].iloc[0]\n",
    "print(f\"üìç GPS station {station} at lat={lat}, lon={lon}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2ee42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.errorbar(df_gps['decimal_year'], df_gps['up'], yerr=df_gps['sig_u'],\n",
    "             fmt='o', markersize=3, capsize=2, label='Up displacement', ecolor='gray')\n",
    "plt.xlabel('Decimal Year')\n",
    "plt.ylabel('Displacement (mm)')\n",
    "plt.title('Vertical Displacement (Up) - GPS Station MEDI')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db6d15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import cdsapi\n",
    "\n",
    "c = cdsapi.Client()\n",
    "\n",
    "for year in range(2016, 2026):\n",
    "    outfile = data_dir / f\"era5_tcwv_{year}.nc\"\n",
    "    if outfile.exists():\n",
    "        print(f\"{outfile.name} already downloaded.\")\n",
    "        continue\n",
    "\n",
    "    c.retrieve(\n",
    "        'reanalysis-era5-single-levels',\n",
    "        {\n",
    "            'product_type': 'reanalysis',\n",
    "            'variable': ['total_column_water_vapour'],\n",
    "            'year': str(year),\n",
    "            'month': [f\"{m:02d}\" for m in range(1, 13)],\n",
    "            'day': [f\"{d:02d}\" for d in range(1, 32)],\n",
    "            'time': ['12:00'],\n",
    "            'format': 'netcdf',\n",
    "            'area': [44.6, 11.2, 44.4, 11.5],  # [north, west, south, east]\n",
    "        },\n",
    "        str(outfile)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d27fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/ubuntu/tools/MintPy/src')\n",
    "\n",
    "from mintpy.cli import prep_hyp3\n",
    "from mintpy.utils import readfile, writefile\n",
    "from pathlib import Path\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcaad06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (f\"Data directory: {data_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b62dce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mintpy.utils import writefile\n",
    "print(dir(writefile))  # check if 'write_roipac_rsc' is listed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894245e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mintpy import prep_hyp3\n",
    "from mintpy.utils import readfile, writefile\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "# Define file patterns to include\n",
    "patterns = [\n",
    "    \"*unw_phase*.tif\",\n",
    "    \"*corr*.tif\",\n",
    "    \"*dem*.tif\",\n",
    "    \"*lv_theta*.tif\"\n",
    "]\n",
    "\n",
    "# Collect all matching files\n",
    "all_tifs = []\n",
    "for pat in patterns:\n",
    "    all_tifs.extend(data_dir.rglob(pat))\n",
    "\n",
    "# Loop through and write .rsc for each\n",
    "for tif_file in sorted(all_tifs):\n",
    "    try:\n",
    "        print(f\"Processing: {tif_file.name}\")\n",
    "        meta = readfile.read_gdal_vrt(str(tif_file))\n",
    "        meta = prep_hyp3.add_hyp3_metadata(str(tif_file), meta)\n",
    "        writefile.write_roipac_rsc(meta, str(tif_file) + \".rsc\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed on {tif_file.name}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c882b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "mintpy_input_dir = Path(\"/home/ubuntu/work/aoi_bologna/aoi_3_1/mintpy_inputs\")\n",
    "mintpy_input_dir.mkdir(exist_ok=True)\n",
    "\n",
    "cmd = f\"\"\"\n",
    "prep_hyp3.py \\\n",
    "--dir \"{data_dir}\" \\\n",
    "--outdir \"{mintpy_input_dir}\" \\\n",
    "--file \"**/*unw_phase_clipped.tif\" \\\n",
    "--coherence \"**/*corr_clipped.tif\" \\\n",
    "--ex \"**/*dem_clipped.tif\" \\\n",
    "--theta \"**/*lv_theta_clipped.tif\"\n",
    "\"\"\"\n",
    "\n",
    "subprocess.run(cmd, check=True, shell=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc999ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import List, Union\n",
    "from osgeo import gdal\n",
    "\n",
    "def get_common_overlap(file_list: List[Union[str, Path]]) -> List[float]:\n",
    "    \"\"\"Get the common overlap of GeoTIFF files with correct projWin bounds.\"\"\"\n",
    "    corners = [gdal.Info(str(file), format='json')['cornerCoordinates'] for file in file_list]\n",
    "\n",
    "    ulx = min(corner['upperLeft'][0] for corner in corners)\n",
    "    uly = max(corner['upperLeft'][1] for corner in corners)\n",
    "    lrx = max(corner['lowerRight'][0] for corner in corners)\n",
    "    lry = min(corner['lowerRight'][1] for corner in corners)\n",
    "\n",
    "    \n",
    "\n",
    "    return [ulx, uly, lrx, lry]\n",
    "\n",
    "\n",
    "'''\n",
    "def get_common_overlap(file_list: List[Union[str, Path]]) -> List[float]:\n",
    "    \"\"\"Get the common overlap of  a list of GeoTIFF files\n",
    "    \n",
    "    Arg:\n",
    "        file_list: a list of GeoTIFF files\n",
    "    \n",
    "    Returns:\n",
    "         [ulx, uly, lrx, lry], the upper-left x, upper-left y, lower-right x, and lower-right y\n",
    "         corner coordinates of the common overlap\n",
    "    \"\"\"\n",
    "    \n",
    "    corners = [gdal.Info(str(dem), format='json')['cornerCoordinates'] for dem in file_list]\n",
    "\n",
    "    ulx = max(corner['upperLeft'][0] for corner in corners)\n",
    "    uly = min(corner['upperLeft'][1] for corner in corners)\n",
    "    lrx = min(corner['lowerRight'][0] for corner in corners)\n",
    "    lry = max(corner['lowerRight'][1] for corner in corners)\n",
    "    return [ulx, uly, lrx, lry]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa219e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from osgeo import gdal\n",
    "gdal.UseExceptions()\n",
    "data_dir = Path(data_dir)\n",
    "files = data_dir.glob('*/*_dem.tif')\n",
    "\n",
    "overlap = get_common_overlap(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37bb4c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(overlap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f524c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Modify later!\n",
    "\n",
    "from osgeo import gdal\n",
    "\n",
    "ds = gdal.Open(\"/path/to/any_dem_or_unw_phase_file.tif\")\n",
    "proj = ds.GetProjection()\n",
    "print(gdal.Info(ds, format=\"json\")['coordinateSystem'])\n",
    "\n",
    "from shapely.geometry import box\n",
    "from shapely.ops import transform\n",
    "from pyproj import Transformer\n",
    "\n",
    "# Define AOI in lon/lat (WGS84)\n",
    "aoi_wgs84 = box(11.2, 44.4, 11.5, 44.6)\n",
    "\n",
    "transformer = Transformer.from_crs(\"EPSG:4326\", \"EPSG:32633\", always_xy=True)\n",
    "aoi_utm33 = transform(transformer.transform, aoi_wgs84)\n",
    "\n",
    "print(\"AOI in UTM 33N (EPSG:32633):\", aoi_utm33.bounds)\n",
    "(minx, miny, maxx, maxy) = aoi_utm33.bounds\n",
    "# Equivalent to [ulx, uly, lrx, lry] for your AOI in meters\n",
    "from shapely.geometry import box\n",
    "\n",
    "overlap_box = box(overlap[0], overlap[3], overlap[2], overlap[1])  # minx, miny, maxx, maxy\n",
    "print(\"Does overlap contain AOI?\", overlap_box.contains(aoi_utm33))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b914ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check this alternative later ... !\n",
    "# Clip all GeoTIFF files to their common overlap\n",
    "from pathlib import Path\n",
    "from typing import List, Union\n",
    "from osgeo import gdal\n",
    "from tqdm import tqdm\n",
    "\n",
    "def clip_hyp3_products_to_common_overlap(data_dir: Union[str, Path], overlap: List[float]) -> None:\n",
    "    \"\"\"Clip all GeoTIFF files to their common overlap\n",
    "\n",
    "    Args:\n",
    "        data_dir: directory containing the GeoTIFF files to clip\n",
    "        overlap: [ulx, uly, lrx, lry] of the common overlap area\n",
    "    Returns: None\n",
    "    \"\"\"\n",
    "\n",
    "    data_dir = Path(data_dir)\n",
    "    files_for_mintpy = ['_water_mask.tif', '_corr.tif', '_unw_phase.tif', '_dem.tif', '_lv_theta.tif', '_lv_phi.tif']\n",
    "\n",
    "    for extension in files_for_mintpy:\n",
    "        matching_files = list(data_dir.rglob(f'*{extension}'))\n",
    "        for file in tqdm(matching_files, desc=f'Clipping {extension} files'):\n",
    "            dst_file = file.parent / f'{file.stem}_clipped{file.suffix}'\n",
    "            gdal.Translate(destName=str(dst_file), srcDS=str(file), projWin=overlap)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91ad584",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import List, Union\n",
    "\n",
    "def clip_hyp3_products_to_common_overlap(data_dir: Union[str, Path], overlap: List[float]) -> None:\n",
    "    \"\"\"Clip all GeoTIFF files to their common overlap\n",
    "    \n",
    "    Args:\n",
    "        data_dir:\n",
    "            directory containing the GeoTIFF files to clip\n",
    "        overlap:\n",
    "            a list of the upper-left x, upper-left y, lower-right-x, and lower-tight y\n",
    "            corner coordinates of the common overlap\n",
    "    Returns: None\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    files_for_mintpy = ['_water_mask.tif', '_corr.tif', '_unw_phase.tif', '_dem.tif', '_lv_theta.tif', '_lv_phi.tif']\n",
    "\n",
    "    for extension in files_for_mintpy:\n",
    "\n",
    "        for file in data_dir.rglob(f'*{extension}'):\n",
    "\n",
    "            dst_file = file.parent / f'{file.stem}_clipped{file.suffix}'\n",
    "\n",
    "            gdal.Translate(destName=str(dst_file), srcDS=str(file), projWin=overlap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda90839",
   "metadata": {},
   "outputs": [],
   "source": [
    "from osgeo import gdal\n",
    "gdal.UseExceptions()\n",
    "\n",
    "clip_hyp3_products_to_common_overlap(data_dir, overlap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09bab56",
   "metadata": {},
   "outputs": [],
   "source": [
    "mintpy_config = work_dir / 'mintpy_config.txt'\n",
    "mintpy_config.write_text(\n",
    "f\"\"\"\n",
    "##---------processor info:\n",
    "mintpy.load.processor        = hyp3\n",
    "\n",
    "##---------interferogram datasets:\n",
    "mintpy.load.unwFile          = {data_dir}/*/*_unw_phase_clipped.tif\n",
    "mintpy.load.corFile          = {data_dir}/*/*_corr_clipped.tif\n",
    "\n",
    "##---------geometry datasets:\n",
    "mintpy.load.demFile          = {data_dir}/*/*_dem_clipped.tif\n",
    "mintpy.load.incAngleFile     = {data_dir}/*/*_lv_theta_clipped.tif\n",
    "mintpy.load.azAngleFile      = {data_dir}/*/*_lv_phi_clipped.tif\n",
    "mintpy.load.waterMaskFile    = {data_dir}/*/*_water_mask_clipped.tif\n",
    "\n",
    "##---------tropospheric delay correction:\n",
    "mintpy.troposphericDelay.method        = weatherModel\n",
    "mintpy.weatherModel.weatherModel       = ERA5\n",
    "mintpy.weatherModel.weatherDir         = {data_dir}\n",
    "mintpy.weatherModel.weatherFile        = era5_tcwv_*.nc\n",
    "mintpy.weatherModel.heightRef          = auto\n",
    "\n",
    "##---------additional options:\n",
    "mintpy.network.inBaseline              = yes\n",
    "mintpy.network.maxTempBaseline         = 180\n",
    "mintpy.network.maxBTempBaseline        = 1500\n",
    "\n",
    "mintpy.unwrapMask.connCompMinPix       = 100\n",
    "mintpy.unwrapMask.maskFile             = auto\n",
    "\n",
    "mintpy.waterBody.mask                  = auto\n",
    "mintpy.reference.pixelMethod           = lonlat\n",
    "mintpy.reference.lalo                  = 44.5199558120, 11.6468120447  # MEDI GPS lat, lon (adjusted from -348¬∞ to +11.65¬∞)\n",
    "mintpy.reference.interpMethod          = linear\n",
    "mintpy.tempCohMask.min                 = 0.7\n",
    "mintpy.deramp                          = quadratic\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7501675",
   "metadata": {},
   "outputs": [],
   "source": [
    "!smallbaselineApp.py --dir {work_dir} {mintpy_config}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b9a62f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4d8f8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "isce2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
